{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tesfami1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tesfami1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Requirement already satisfied: contractions in /Users/tesfami1/anaconda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import re, string, unicodedata\n",
    "import sys\n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions\n",
    "\n",
    "PATH = '../data/Reviews.csv'\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def normalize(words):\n",
    "    \"\"\"Apply the above functions\"\"\"\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "#     words = remove_stopwords(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Progess(80%) XGBoost Model (using only 30,000 rows for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class XGBoostModel:\n",
    "    def __init__(self, vectorizer, path=''):\n",
    "        \"\"\"read data as a dataframe\"\"\"\n",
    "        self.path = path or PATH\n",
    "        self.data = pd.read_csv(self.path)\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.vect = None\n",
    "        self.vect_type = vectorizer\n",
    "        self.model = None\n",
    "        \n",
    "    # Data pre-proccessing section\n",
    "    \n",
    "    def __prepareData__(self):\n",
    "        \"\"\"remove unneccessary columns and rows\"\"\"\n",
    "        self.data.dropna(inplace=True)\n",
    "        self.data = self.data.iloc[:30000,:]\n",
    "        self.data = self.data[self.data['Score'] != 3]\n",
    "        self.data = self.data[self.data['HelpfulnessNumerator']!=0]\n",
    "        self.data['Positivity'] = np.where(self.data['Score'] > 3, 1, 0)\n",
    "      \n",
    "    def __splitDataFromTarget__(self):\n",
    "        \"\"\"split target column from the given data\"\"\"\n",
    "        self.__prepareData__()\n",
    "        X,y = self.data.iloc[:,:-1], self.data.iloc[:,-1]\n",
    "        return X, y\n",
    "    \n",
    "    def __trainTestSplit__(self):\n",
    "        \"\"\"split data into training and testing data\"\"\"\n",
    "        X, y = self.__splitDataFromTarget__()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X['Text'], y, test_size = 0.2, random_state = 15)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def tokenizeNormalize(self, string):\n",
    "        \"\"\"chop each statement into words but before that fix contactions\"\"\"\n",
    "        string = replace_contractions(string)\n",
    "        words = nltk.word_tokenize(string)\n",
    "        return normalize(words)\n",
    "\n",
    "    def count_vectorizer(self, data):\n",
    "        \"\"\"Using simple counting to prepare bag of words\"\"\"\n",
    "        stop_words = ['this','that','a','i','he','she','they','we']\n",
    "#         vectorizer = CountVectorizer(min_df = 1, ngram_range=(1,2), stop_words='english', tokenizer=self.tokenizeNormalize)\n",
    "        vectorizer = CountVectorizer(min_df = 1, ngram_range=(1,2), stop_words=stop_words)\n",
    "        self.vect = vectorizer.fit(self.X_train)\n",
    "        data_vectorized = self.vect.transform(data)\n",
    "        return data_vectorized\n",
    "    \n",
    "    def tf_idf_vectorizer(self, data):\n",
    "        \"\"\"Use the TF*IDF method to prepare bag of words\"\"\"\n",
    "#         tf_vect = TfidfVectorizer(stop_words=stopwords.words('english'), tokenizer=self.tokenizeNormalize).fit(X_train)\n",
    "        stop_words = ['this','that','a','i','he','she','they','we']\n",
    "        self.vect = TfidfVectorizer(stop_words=stop_words,tokenizer=self.tokenizeNormalize).fit(self.X_train)\n",
    "        data_tf_vectorized = self.vect.transform(data)\n",
    "        return data_tf_vectorized\n",
    "    \n",
    "    def quickView(self):\n",
    "        \"\"\"nice table view of first 5 rows of the data\"\"\"\n",
    "        self.data.head()\n",
    "        \n",
    "    \n",
    "    #XGBoost provides a wrapper class to allow models \n",
    "    #to be treated like classifiers or regressors in the scikit-learn framework.\n",
    "    #Models are fit using the scikit-learn API and the model.fit() function.\n",
    "    \n",
    "\n",
    "    # Model trainer and testing functions\n",
    "    \n",
    "    def plain_train(self,params={}):\n",
    "        \"\"\"train model without cross-validations.\"\"\"\n",
    "        self.__trainTestSplit__()\n",
    "        self.X_train = self.count_vectorizer(self.X_train) if self.vect_type == 'COUNT' else \\\n",
    "                        (self.tf_idf_vectorizer(self.X_train) if self.vect_type == 'TF_IDF' else None)\n",
    "        if self.X_train is None:\n",
    "            raise ValueError('vectorizer type cannot be none. It should be COUNT or TF_IDF.')\n",
    "        else:                                                                        \n",
    "            params = params or {'objective':'binary:logistic', 'colsample_bytree':0.3, 'max_depth':5, 'reg_alpha':10}\n",
    "            self.model = xgb.XGBClassifier(**params)\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "            print('Training models without cross-validation finished. Run tests!')\n",
    "\n",
    "    def test_model(self):\n",
    "        \"\"\"test model and compute error\"\"\"\n",
    "        if self.model is not None:\n",
    "            predictions = self.model.predict(self.vect.transform(self.X_test))\n",
    "            print(f'Accuracy: {(self.y_test==predictions).sum()*100/self.y_test.shape[0]}%')\n",
    "        else:\n",
    "            print(\"Error: prepare data and train model first. :[ \")\n",
    "    def cv_train(self,params={}):\n",
    "        \"\"\"train model with 3-fold cross-validations\"\"\"\n",
    "        X, y = __splitData__()\n",
    "        data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "        params = params or {}\n",
    "        cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=123)\n",
    "        print(cv_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 (when using simple count vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models without cross-validation finished. Run tests!\n"
     ]
    }
   ],
   "source": [
    "# model test using simple count_vectorizer\n",
    "model = XGBoostModel(\"COUNT\")\n",
    "model.plain_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tesfami1/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with user input corpus\n",
    "test_corpus = ['I like this product a lot.', 'I love it', 'It is so good','I will not buy it again', 'I hate this product. I am never going to get it']\n",
    "model.model.predict(model.vect.transform(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.05050505050505%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tesfami1/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# testing using x_test\n",
    "model.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 (when using TF_IDF vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tesfami1/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models without cross-validation finished. Run tests!\n"
     ]
    }
   ],
   "source": [
    "#model test using td*idf vectorizer\n",
    "model2 = XGBoostModel(\"TF_IDF\")\n",
    "model2.plain_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tesfami1/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "/Users/tesfami1/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus2 = ['I like this product a lot.', 'I hate this product. I am never going to get it again', 'I love it', 'It is so good','I will not buy it again']\n",
    "model2.model.predict(model2.vect.transform(test_corpus2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.1010101010101%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tesfami1/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "/Users/tesfami1/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# testing using x_test\n",
    "model2.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3 ( when 3-fold cross-validation is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Progress (data visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class for Exploratory Data Analysis\n",
    "#In Progress\n",
    "class ED_analysis:\n",
    "    def __init__(self, path=''):\n",
    "        self.dframe = pd.read_csv(path or PATH)\n",
    "        \n",
    "    #prepare data for feature analysis\n",
    "    \n",
    "    def __preprocess__(self):\n",
    "        self.dframe.dropna(inplace=True)\n",
    "    def dist(self,colName,title):\n",
    "        # Histogram of var\n",
    "        #sns.distplot(data[colName].dropna());\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.hist(self.dframe[colName].dropna(), bins = 100, edgecolor = 'k')\n",
    "        plt.xlabel(colName)\n",
    "        plt.ylabel('Number of Reviews')\n",
    "        plt.title(title)\n",
    "    def scatterPlot(self);\n",
    "    def densityPlot(self,colName,catVar=None);\n",
    "    def parisPlot(self);\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
