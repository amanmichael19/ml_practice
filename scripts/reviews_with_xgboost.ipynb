{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the `score` assigned with each text amazon fine food reviews using xgboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\dena1\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\dena1\\anaconda3\\lib\\site-packages (from wordcloud) (1.15.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\dena1\\anaconda3\\lib\\site-packages (from wordcloud) (5.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\dena1\\anaconda3\\lib\\site-packages (0.80)\n",
      "Requirement already satisfied: scipy in c:\\users\\dena1\\anaconda3\\lib\\site-packages (from xgboost) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dena1\\anaconda3\\lib\\site-packages (from xgboost) (1.15.1)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dena1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dena1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wordcloud\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # colorful plots\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score #evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time #check runtime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Constants and Configurations\n",
    "\n",
    "DATA_LOCAL = True\n",
    "LOCAL_PATH = \"../data/\"\n",
    "REMOTE_PATH = \"https://s3.amazonaws.com/coetichr/AmazonFoodReviews/\"\n",
    "NROWS = 1000 #edit with a smaller size to develop faster\n",
    "TOKENIZER = RegexpTokenizer(r'\\w+') #no punctuation\n",
    "STEMMER = nltk.PorterStemmer()\n",
    "STOPWORD_CORPUS = stopwords.words(\"english\")\n",
    "COLUMNS_TO_DROP = [\"ProductId\", \"UserId\", \"ProfileName\", \"Time\", \"Summary\", \"HelpfulnessDenominator\", \"HelpfulnessNumerator\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the data from the csv file\n",
    "@param file Filename to load\n",
    "@param nrow_value Number of rows to load\n",
    "@reutrn dataframe\n",
    "'''\n",
    "def load_data(file, nrow_value=NROWS):\n",
    "    path = LOCAL_PATH if DATA_LOCAL else REMOTE_PATH\n",
    "    return pd.read_csv(path + file, nrows=nrow_value)\n",
    "\n",
    "'''\n",
    "Sets up the dataframe with the correct configs and positivity scores\n",
    "@param nrows Number of rows if not default NROWS constant\n",
    "@return dataframe\n",
    "'''\n",
    "def setup_df(nrows):\n",
    "        # Load Reviews.csv\n",
    "    reviewsdf = load_data(\"Reviews.csv\", nrows)\n",
    "\n",
    "    #drop columns not needed\n",
    "    reviewsdf.drop(columns=COLUMNS_TO_DROP, inplace=True, axis=1)\n",
    "    reviewsdf.dropna(inplace=True) #drop empty fields\n",
    "    reviewsdf['Positivity'] = np.where(reviewsdf['Score'] > 3, 1, 0)\n",
    "\n",
    "    print(\"Original DF Shape: \", reviewsdf.shape)\n",
    "    print(reviewsdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DF Shape:  (1000, 4)\n",
      "   Id  Score                                               Text  Positivity\n",
      "0   1      5  I have bought several of the Vitality canned d...           1\n",
      "1   2      1  Product arrived labeled as Jumbo Salted Peanut...           0\n",
      "2   3      4  This is a confection that has been around a fe...           1\n",
      "3   4      2  If you are looking for the secret ingredient i...           0\n",
      "4   5      5  Great taffy at a great price.  There was a wid...           1\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "reviewsdf = setup_df(NROWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Look at the distribution of different scores and words received for each text review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class to quickly generate a plot with the correct settings for this data\n",
    "'''\n",
    "class QuickPlot:\n",
    "    \n",
    "    '''\n",
    "    constructor\n",
    "    @param title Title of the plot to display\n",
    "    @param sns_style Color of background for seaborn plots\n",
    "    '''\n",
    "    def __init__(self, title=\"\"):\n",
    "        sns.set(style=\"whitegrid\", context=\"paper\")\n",
    "        plt.subplots(figsize=(10,5))\n",
    "        self.title = title;\n",
    "        \n",
    "    '''\n",
    "    Generates a wordcloud from frequencies\n",
    "    @param hash Dictionary mapping the word string to the number/size\n",
    "    '''\n",
    "    def wordcloud(self, hash):\n",
    "        wordcloud = WordCloud(\n",
    "            width=1400,\n",
    "            height=600,\n",
    "            background_color=\"white\",\n",
    "            colormap=\"Dark2\",\n",
    "            max_words=50,\n",
    "            mode=\"RGBA\").generate_from_frequencies(hash)\n",
    "        \n",
    "        # plot words\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "    '''\n",
    "    Creates a bar plot with a dataframe\n",
    "    @param df Dataframe to use\n",
    "    @param xcol Column for x axis\n",
    "    @param ycol Column for y axis\n",
    "    '''\n",
    "    def barplot(self, df, xcol=\"count\", ycol=\"token\"):\n",
    "        sns.barplot(x = df[xcol], y = df[ycol], palette=\"deep\").set_title(self.title)\n",
    "        plt.show() \n",
    "        \n",
    "    '''\n",
    "    Createa a count plot, where seaborn aggregates the data\n",
    "    @param df dataframe\n",
    "    @param xcol X axis to show counts for\n",
    "    '''\n",
    "    def countplot(self, df, xcol):\n",
    "        sns.countplot(x=xcol, data=df, palette=\"deep\").set_title(self.title)\n",
    "        plt.show()\n",
    "        \n",
    "    '''\n",
    "    Creates a line plot \n",
    "    @param df dataframe\n",
    "    @param xcol X axis to select from dataframe\n",
    "    @param ycol Y axis to select from dataframe\n",
    "    '''\n",
    "    def lineplot(self, df, xcol, ycol):\n",
    "        sns.lineplot(x=xcol, y=ycol, hue=\"GnBu_d\", data=df).set_title(self.title)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view distribution of scores\n",
    "\n",
    "#QuickPlot(\"Score Distribution\").countplot(reviewsdf, \"Score\")\n",
    "\n",
    "#view top words for each score\n",
    "#sum_words = bow.sum(axis=0) \n",
    "#words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "#word_freq_dict = {}\n",
    "#for a, b in words_freq: \n",
    "#    word_freq_dict[a] = float(b)\n",
    "#QuickPlot().wordcloud(word_freq_dict)\n",
    "\n",
    "#words_freq_df = pd.DataFrame(word_freq_dict.items(),index=word_freq_dict.keys(), columns=[\"token\", \"count\"])\n",
    "#QuickPlot(\"Top 25 words in reviews\").barplot(words_freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Going to tokenize the words and anything that might need to be removed (stopwords, puncutation, etc), apply stemming where the root of the word is extracted from the sentence, and create a bag of words model. BoW representation specifies the occurrence of tokens in the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parse the text into tokens and apply to the table, along with stemming\n",
    "@param row dataframe row to apply\n",
    "@return array of tokens\n",
    "'''\n",
    "def tokenizefn(row):\n",
    "    rownew = TOKENIZER.tokenize(row) #tokenize    \n",
    "    #stem\n",
    "    #rownew = list(map(lambda str: STEMMER.stem(str), rownew))\n",
    "    return rownew\n",
    "\n",
    "'''\n",
    "setup the vectorizer with the params\n",
    "@param train Training data to fit the vectorizer on\n",
    "@param ngramrange Max range for ngram size\n",
    "@param mindf_value Minimum value to allow for the vectorizer\n",
    "@param is_lowercase boolean to make everything lower case or not\n",
    "@return Countvectorizer\n",
    "'''\n",
    "def vectorizer_setup(train, ngramrange, mindf_value, is_lowercase):    \n",
    "    #build BoW with different nfram ranges\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range = (1, ngramrange),\n",
    "        analyzer = 'word',\n",
    "        tokenizer=tokenizefn,\n",
    "        lowercase=is_lowercase,\n",
    "        min_df=mindf_value\n",
    "        ).fit(train)\n",
    "\n",
    "    print(\"Number of features: \" + str(len(vectorizer.get_feature_names())))\n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-2fe6850bb663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviewsdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreviewsdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Positivity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_setup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Set: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test Set: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(reviewsdf[\"Text\"], reviewsdf['Positivity'], random_state = 0)\n",
    "vectorizer = vectorizer_setup(X_train, 6, 10, False)\n",
    "print(\"Training Set: \", X_train.shape)\n",
    "print(\"Test Set: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test and train a model with the given params\n",
    "@param x_train\n",
    "@param x_test\n",
    "@param y_train\n",
    "@param y_test\n",
    "@param vectorizer CountVecctorizer\n",
    "'''\n",
    "class ModelRun():\n",
    "    def __init__(self, x_train, x_test, y_train, y_test, vectorizer):\n",
    "        self.xtrain = x_train\n",
    "        self.xtest = x_test\n",
    "        self.ytrain = y_train\n",
    "        self.ytest = y_test\n",
    "        self.vec = vectorizer\n",
    "        \n",
    "    '''\n",
    "    private function\n",
    "    Test the accuracy, precision, and recall of a given classifier\n",
    "    @param classfiier Classifier to run\n",
    "    '''\n",
    "    def __test_classifier(self, classifier):\n",
    "        print(\"\")\n",
    "        print(\"===============================================\")\n",
    "        classifier_name = str(type(classifier).__name__)\n",
    "        print(\"Testing \" + classifier_name)\n",
    "        now = time()\n",
    "        list_of_labels = sorted(list(set(self.ytrain)))\n",
    "        model = classifier.fit(self.vec.transform(self.xtrain), y_train)\n",
    "        print(\"Learing time {0}s\".format(time() - now))\n",
    "        now = time()\n",
    "        predictions = model.predict(self.vec.transform(self.xtest))\n",
    "        print(\"Predicting time {0}s\".format(time() - now))\n",
    "\n",
    "        precision = precision_score(self.ytest, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "        recall = recall_score(self.ytest, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "        accuracy = accuracy_score(self.ytest, predictions)\n",
    "        f1 = f1_score(self.ytest, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "        print(\"=================== Results ===================\")\n",
    "        print(\"            Negative     Neutral     Positive\")\n",
    "        print(\"F1       \" + str(f1))\n",
    "        print(\"Precision\" + str(precision))\n",
    "        print(\"Recall   \" + str(recall))\n",
    "        print(\"Accuracy \" + str(accuracy))\n",
    "        print(\"===============================================\")\n",
    "\n",
    "        return precision, recall, accuracy, f1\n",
    "\n",
    "    '''\n",
    "    LogisticRegressioni model\n",
    "    @param multiclass_val\n",
    "    @param solver_val\n",
    "    @return precision, recall, accuracy, f1\n",
    "    '''\n",
    "    def logistic_regression_run(self, multiclass_val='ovr', solver_val='liblinear'):\n",
    "        return self.__test_classifier(\n",
    "            LogisticRegression(\n",
    "                random_state=0,\n",
    "                solver=solver_val,\n",
    "                multi_class=multiclass_val,\n",
    "                n_jobs=-1))\n",
    "\n",
    "    '''\n",
    "    XGBClassifier model\n",
    "    @param max_depth_val\n",
    "    @param n_estimators_val\n",
    "    @return precision, recall, accuracy, f1\n",
    "    '''\n",
    "    def xgb_classifier_run(self, max_depth_val=50, n_estimators_val=1000):\n",
    "        return self.__test_classifier(\n",
    "        xgb.XGBClassifier(\n",
    "            silent=True,\n",
    "            max_depth=max_depth_val,\n",
    "            n_jobs=4,\n",
    "            n_estimators=n_estimators_val\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "Testing LogisticRegression\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-3190ca3529db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mModelRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogistic_regression_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-a9506c1308f3>\u001b[0m in \u001b[0;36mlogistic_regression_run\u001b[1;34m(self, multiclass_val, solver_val)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msolver_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulticlass_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 n_jobs=-1))\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     '''\n",
      "\u001b[1;32m<ipython-input-79-a9506c1308f3>\u001b[0m in \u001b[0;36m__test_classifier\u001b[1;34m(self, classifier)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mlist_of_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Learing time {0}s\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "ModelRun(X_train, X_test, Y_train, Y_test, vectorizer).logistic_regression_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
